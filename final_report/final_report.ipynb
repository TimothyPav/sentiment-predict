{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c36e128f",
   "metadata": {},
   "source": [
    "# Sentiment Adjusted Stock Prediction\n",
    "By:\n",
    "\n",
    "Date: /2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f38cc9",
   "metadata": {},
   "source": [
    "With the virtually infinite amount of information floating around on social media about current world affairs, it would be nice to be able to automatically parse the information and get the data you need related to which investments you should buy. We can solve this problem by building a program to automatically scrape the data and derive key details from it using AI.\n",
    "\n",
    "We believe that we can get a better picture of what stocks will be if we can combine 2 things, well optimized stock data and a twitter api. The first part of this report will focus on the:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d5f355",
   "metadata": {},
   "source": [
    "# Stock Data Collection\n",
    "\n",
    "Large detailed stock datasets were hard to come by, so we had to come up with a strategy to build a large enough dataset within a few acceptance criteria:\n",
    "\n",
    "- A large array of stocks, given in this project as \"stock_tickers.csv\" or S\n",
    "- Stocks that were going to stay relatively stable, for this I selected a list of the companies with the largest market cap descending, and collected as large a data set as possible $(\\max(size(S)))$\n",
    "- Stocks that were freely available with the data from Alpha Vantage (the API used in this project) set of Alpha Vantage given $A$, condition $S \\in A$\n",
    "- Stocks will be picked from the top of the Nasdaq top stock list $S_i = N_i, i = {1,2,3,\\dots,n}$\n",
    "- Number of pulls the API will allow me to do from Wall Street close on Friday to open on Monday (don't know how to express this one as the API throttling seems to happen at random)\n",
    "\n",
    "Provided mathematically as:\n",
    "\n",
    "$$\\max(size(S))$$\n",
    "\n",
    "$$s.t.$$\n",
    "\n",
    "$$S \\in A$$\n",
    "\n",
    "$$S_i = N_i, i = \\{ 1,2,3,\\dots,n \\}$$\n",
    "\n",
    "n = the number of entries I am able to pull during the weekend the NYSE is closed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109b1188",
   "metadata": {},
   "source": [
    "## $$S_i = N_i, i = \\{ 1,2,3,\\dots,n \\}$$\n",
    "This was a very simple constraint to follow because it just required me to search the NASDAQ for the highest market cap companies and download the 6000 entry long csv of stock valuation and tickers, that I can isolate the tickers on ([nasdq.com/highest market cap](https://www.nasdaq.com/market-activity/index/spx/historical?page=1&rows_per_page=10&timeline=m1)). This data could not be directly used as the stock market is in a state of flux which necessitates a higher amount of granularity see constraint below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3283621",
   "metadata": {},
   "source": [
    "## $$S \\in A$$\n",
    "This project relies upon the stock data that can be freely procured from the Alpha Vantage API ([alphavantage.co/documentation](https://www.alphavantage.co/documentation/)), this constraint wa probed by using the above constraint set, using error handling for checking if it wasn't in the set (after an 8 hour timeout, that was found to be enough for the API timeout to reset) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cef3d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "max_retries = 5\n",
    "\n",
    "...\n",
    "\n",
    "if not time_series:\n",
    "            print(f\"No time series data found for {symbol}.\") #bad data catch\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf68984b",
   "metadata": {},
   "source": [
    "## $$\\max(size(S))$$\n",
    "This function is more served as me running a few scripts periodically throughout the weekend to maximize the number of stocks that can be grabbed from the API. It is run wit this .bat file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c54ba8a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@echo off\n",
    "\n",
    "echo Starting up the app...\n",
    "\n",
    "python -m compileall -q .\n",
    "\n",
    "python main.py\n",
    "\n",
    "pause"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03f712e",
   "metadata": {},
   "source": [
    "But the pertinent code for this report and this class is this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b143ad37",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for attempt in range(max_retries):\n",
    "            for i, key in enumerate(keys):\n",
    "                url = f'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol={symbol}&interval=30min&apikey={key}' ## url to alphavantage\n",
    "                response = requests.get(url)\n",
    "                data = response.json()\n",
    "\n",
    "                if \"Meta Data\" in data and \"Time Series (30min)\" in data:\n",
    "                    print(f\"[Key {i + 1}] Success for {symbol}.\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"[Key {i + 1}] failed or rate-limited. Trying next key...\") ## catch if they are limiting my rates (although I am starting to think it is by static ip)\n",
    "\n",
    "            else:\n",
    "                print(f\"[Attempt {attempt + 1}] All keys failed. Waiting {retry_delay / 60} minutes...\")\n",
    "                if attempt == 3: # ratchet catch to get all the data I could glean so far, will overwrite if I get more data\n",
    "                    downloader = stockCSVDownloader()\n",
    "                    downloader.move_to_downloads(\"stocks.csv\") #actual downloader\n",
    "\n",
    "                    printer = lastStockPrinter()\n",
    "                    printer.move_last_stock_to_downloads(symbol, str(self.count))\n",
    "                    continue\n",
    "                elif attempt == 4:\n",
    "                    time.sleep(retry_delay * 8) #long sleeper so I can afk this\n",
    "\n",
    "                time.sleep(retry_delay)\n",
    "                continue\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for {symbol} after {max_retries} retries.\")\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a766b",
   "metadata": {},
   "source": [
    "This peice of code queries the API and adds the stock data to the csv that will be staged and downloaded after 3 failed attempts (my \"soft catch\" that hopefully catches when the API starts to throttle me.) after it throttles me, it prints out the last stock to be queried, I turn on a vpn, and continue the process from the last stock ticker it printed out.\n",
    "\n",
    "All stocks_# are then combined and fed into a variance calculator made in Julia, given below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e0b827",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "using Statistics\n",
    "\n",
    "df = CSV.read(\"top_stocks_data.csv\", DataFrame)\n",
    "grouped = groupby(df, :ticker)\n",
    "\n",
    "results = DataFrame(\n",
    "    ticker = String[],\n",
    "    open_variance = Float64[],\n",
    "    high_variance = Float64[],\n",
    "    low_variance = Float64[],\n",
    "    close_variance = Float64[],\n",
    "    volume_variance = Float64[],\n",
    ")\n",
    "\n",
    "for g in grouped\n",
    "    push!(results, (\n",
    "        ticker = first(g.ticker),\n",
    "        open_variance = var(g.open),\n",
    "        high_variance = var(g.high),\n",
    "        low_variance = var(g.low),\n",
    "        close_variance = var(g.close),\n",
    "        volume_variance = var(g.volume),\n",
    "    ))\n",
    "end\n",
    "\n",
    "CSV.write(\"stock_variance.csv\", results)\n",
    "\n",
    "println(\"Variance data was written to stock_variance.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e27467",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
