{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c36e128f",
   "metadata": {},
   "source": [
    "# Sentiment Adjusted Stock Prediction\n",
    "By:\n",
    "\n",
    "Date: /2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f38cc9",
   "metadata": {},
   "source": [
    "With the virtually infinite amount of information floating around on social media about current world affairs, it would be nice to be able to automatically parse the information and get the data you need related to which investments you should buy. We can solve this problem by building a program to automatically scrape the data and derive key details from it using AI.\n",
    "\n",
    "We believe that we can get a better picture of what stocks will be if we can combine 2 things, well optimized stock data and a twitter api. The first part of this report will focus on the:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a98121",
   "metadata": {},
   "source": [
    "# Stock Data\n",
    "\n",
    "Our stock data can be broadly broken down into 2 different categories:\n",
    "\n",
    "- Daily Stock Data\n",
    "- Historical Stock Data\n",
    "\n",
    "This project initially started off by using Daily stock data entirely, but this did not provide enough variance to collect adequate stock data so we had to add historic data. We build out our data matrix by giving a bias of 1:1 daily:historical data, building out a matrix that takes both the volatile nature of todays markets and a matrix that has enough stock variance to make some sort of long term predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db3c91",
   "metadata": {},
   "source": [
    "# Stock Data Collection Historical\n",
    "\n",
    "Historical stock data can be easily gotten from [kaggle/stock_data](https://www.kaggle.com/datasets/jacksoncrow/stock-market-dataset?resource=download) and extracted from our sample number of stocks using the python code below:\n",
    "\n",
    "```python\n",
    "for ticker in tickers_df['tickers']:\n",
    "        ticker = ticker.upper()\n",
    "        filename = f\"{ticker}.csv\"\n",
    "\n",
    "        if not os.path.isfile(filename):\n",
    "            print(f\"Warning: File {filename} does not exist. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Read the individual ticker CSV\n",
    "        df = pd.read_csv(filename)\n",
    "\n",
    "        # Select and rename the columns\n",
    "        df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']].copy()\n",
    "        df.rename(columns={\n",
    "            'Date': 'timestamp',\n",
    "            'Open': 'open',\n",
    "            'High': 'high',\n",
    "            'Low': 'low',\n",
    "            'Close': 'close',\n",
    "            'Volume': 'volume'\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Add the ticker column\n",
    "        df['ticker'] = ticker\n",
    "\n",
    "        # Format timestamp to add '00:00:00'\n",
    "        df['timestamp'] = df['timestamp'] + ' 00:00:00'\n",
    "\n",
    "        # Reorder columns\n",
    "        df = df[['ticker', 'timestamp', 'open', 'high', 'low', 'close', 'volume']]\n",
    "\n",
    "        master_data.append(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec2a27",
   "metadata": {},
   "source": [
    "This then can be fed into a variety of data cleaners, clippers, and modifiers to make the data interface with our combiner later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d5f355",
   "metadata": {},
   "source": [
    "# Stock Data Collection Daily\n",
    "\n",
    "Large detailed stock datasets were hard to come by, so we had to come up with a strategy to build a large enough dataset within a few acceptance criteria:\n",
    "\n",
    "- A large array of stocks, given in this project as \"stock_tickers.csv\" or S\n",
    "- Stocks that were going to stay relatively stable, for this I selected a list of the companies with the largest market cap descending, and collected as large a data set as possible $(\\max(size(S)))$\n",
    "- Stocks that were freely available with the data from Alpha Vantage (the API used in this project) set of Alpha Vantage given $A$, condition $S \\in A$\n",
    "- Stocks will be picked from the top of the Nasdaq top stock list $S_i = N_i, i = {1,2,3,\\dots,n}$\n",
    "- Number of pulls the API will allow me to do from Wall Street close on Friday to open on Monday (don't know how to express this one as the API throttling seems to happen at random)\n",
    "\n",
    "Provided mathematically as:\n",
    "\n",
    "$$\\max(size(S))$$\n",
    "\n",
    "$$s.t.$$\n",
    "\n",
    "$$S \\in A$$\n",
    "\n",
    "$$S_i = N_i, i = \\{ 1,2,3,\\dots,n \\}$$\n",
    "\n",
    "n = the number of entries I am able to pull during the weekend the NYSE is closed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109b1188",
   "metadata": {},
   "source": [
    "## $$S_i = N_i, i = \\{ 1,2,3,\\dots,n \\}$$\n",
    "This was a very simple constraint to follow because it just required me to search the NASDAQ for the highest market cap companies and download the 6000 entry long csv of stock valuation and tickers, that I can isolate the tickers on ([nasdq.com/highest market cap](https://www.nasdaq.com/market-activity/index/spx/historical?page=1&rows_per_page=10&timeline=m1)). This data could not be directly used as the stock market is in a state of flux which necessitates a higher amount of granularity see constraint below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3283621",
   "metadata": {},
   "source": [
    "## $$S \\in A$$\n",
    "This project relies upon the stock data that can be freely procured from the Alpha Vantage API ([alphavantage.co/documentation](https://www.alphavantage.co/documentation/)), this constraint wa probed by using the above constraint set, using error handling for checking if it wasn't in the set (after an 8 hour timeout, that was found to be enough for the API timeout to reset) :\n",
    "\n",
    "```python\n",
    "max_retries = 5\n",
    "\n",
    "...\n",
    "\n",
    "if not time_series:\n",
    "            print(f\"No time series data found for {symbol}.\") #bad data catch\n",
    "            return\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf68984b",
   "metadata": {},
   "source": [
    "## $$\\max(size(S))$$\n",
    "This function is more served as me running a few scripts periodically throughout the weekend to maximize the number of stocks that can be grabbed from the API. It is run wit this .bat file:\n",
    "\n",
    "```bat\n",
    "@echo off\n",
    "\n",
    "echo Starting up the app...\n",
    "\n",
    "python -m compileall -q .\n",
    "\n",
    "python main.py\n",
    "\n",
    "pause\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03f712e",
   "metadata": {},
   "source": [
    "But the pertinent code for this report and this class is this:\n",
    "```python\n",
    "for attempt in range(max_retries):\n",
    "            for i, key in enumerate(keys):\n",
    "                url = f'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol={symbol}&interval=30min&apikey={key}' ## url to alphavantage\n",
    "                response = requests.get(url)\n",
    "                data = response.json()\n",
    "\n",
    "                if \"Meta Data\" in data and \"Time Series (30min)\" in data:\n",
    "                    print(f\"[Key {i + 1}] Success for {symbol}.\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"[Key {i + 1}] failed or rate-limited. Trying next key...\") ## catch if they are limiting my rates (although I am starting to think it is by static ip)\n",
    "\n",
    "            else:\n",
    "                print(f\"[Attempt {attempt + 1}] All keys failed. Waiting {retry_delay / 60} minutes...\")\n",
    "                if attempt == 3: # ratchet catch to get all the data I could glean so far, will overwrite if I get more data\n",
    "                    downloader = stockCSVDownloader()\n",
    "                    downloader.move_to_downloads(\"stocks.csv\") #actual downloader\n",
    "\n",
    "                    printer = lastStockPrinter()\n",
    "                    printer.move_last_stock_to_downloads(symbol, str(self.count))\n",
    "                    continue\n",
    "                elif attempt == 4:\n",
    "                    time.sleep(retry_delay * 8) #long sleeper so I can afk this\n",
    "\n",
    "                time.sleep(retry_delay)\n",
    "                continue\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for {symbol} after {max_retries} retries.\")\n",
    "            return\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a766b",
   "metadata": {},
   "source": [
    "This peice of code queries the API and adds the stock data to the csv that will be staged and downloaded after 3 failed attempts (my \"soft catch\" that hopefully catches when the API starts to throttle me.) after it throttles me, it prints out the last stock to be queried, I turn on a vpn, and continue the process from the last stock ticker it printed out.\n",
    "\n",
    "All stocks_# are then combined and fed into a variance calculator made in Julia, given below:\n",
    "\n",
    "```python\n",
    "using CSV\n",
    "using DataFrames\n",
    "using Statistics\n",
    "\n",
    "df = CSV.read(\"top_stocks_data.csv\", DataFrame)\n",
    "grouped = groupby(df, :ticker)\n",
    "\n",
    "results = DataFrame(\n",
    "    ticker = String[],\n",
    "    open_variance = Float64[],\n",
    "    high_variance = Float64[],\n",
    "    low_variance = Float64[],\n",
    "    close_variance = Float64[],\n",
    "    volume_variance = Float64[],\n",
    ")\n",
    "\n",
    "for g in grouped\n",
    "    push!(results, (\n",
    "        ticker = first(g.ticker),\n",
    "        open_variance = var(g.open),\n",
    "        high_variance = var(g.high),\n",
    "        low_variance = var(g.low),\n",
    "        close_variance = var(g.close),\n",
    "        volume_variance = var(g.volume),\n",
    "    ))\n",
    "end\n",
    "\n",
    "CSV.write(\"stock_variance.csv\", results)\n",
    "\n",
    "println(\"Variance data was written to stock_variance.csv.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae1ff7",
   "metadata": {},
   "source": [
    "# Combining the stock data\n",
    "\n",
    "Because these data sheets (.csv) are theoretically formatted correctly, I combined the via google sheets and uploaded them to then be optimized via our optimizer written in Julia to get the best stocks according to just stock calculation, non-considering the twitter sentiment that will effect the final verdict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4dd63b",
   "metadata": {},
   "source": [
    "# Optimizing the data:\n",
    "The code used to optimize the data can broadly broken down in a few steps\n",
    "#### Uploading data\n",
    "this basically uploads all the data from the .csv\n",
    "```julia\n",
    "# Load stock data\n",
    "stock_data = CSV.read(\"master_stocks.csv\", DataFrame)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9658e9da",
   "metadata": {},
   "source": [
    "#### Calculating returns\n",
    "we need to calculate the approximate daily return using the formula: $$r_t = \\frac{P_t - P_{t-1}}{P_{t-1}}$$\n",
    "where:\n",
    "- $r_t$ is the return at time t\n",
    "- $P_t$ is the price at time t\n",
    "\n",
    "```julia\n",
    "function compute_returns(df)\n",
    "    df = sort(df, :ticker)\n",
    "    returns = [missing; diff(df.close) ./ df.close[1:end-1]]\n",
    "    df[!, :return] = returns\n",
    "    return df\n",
    "end\n",
    "\n",
    "returns_data = combine(groupby(stock_data, :ticker), compute_returns) # Compute returns\n",
    "returns_wide = unstack(returns_data, :ticker, :return; combine=mean)\n",
    "returns_wide_clean = dropmissing(returns_wide) # Drop rows with missing returns\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0853533",
   "metadata": {},
   "source": [
    "#### Calculate covariance Matrix\n",
    "When we have calculated the returns, we can compute the covariance matrix $\\Sigma$ this is a square matrix that describes the covariances between different stocks in the portfolio. This matrix can be given by $$\\sigma_{ij} = \\text{Cov}(r_i, r_j) = \\frac{1}{N-1} \\sum_{t=1}^{N} (r_{i,t} - \\bar{r}_i)(r_{j,t} - \\bar{r}_j)$$\n",
    "```julia\n",
    "returns_matrix = Matrix{Float64}(returns_wide_clean[:, clean_stocks])\n",
    "sigma = cov(returns_matrix)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deed418a",
   "metadata": {},
   "source": [
    "#### Calculate Expected Returns $\\mu$\n",
    "\n",
    "The expected return (given in the vector $\\mu$) is the difference between the open and close which yeilds:$$\\mu_i = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{P_{i,\\text{open},t} - P_{i,\\text{close},t}}{P_{i,\\text{open},t}}$$\n",
    "```julia\n",
    "mu = Float64[]\n",
    "\n",
    "for stock in clean_stocks\n",
    "    timepoints = subset(valid_stock_data, :ticker => ByRow(==(stock)))\n",
    "    expected_return = 0.0\n",
    "    for time in eachrow(timepoints)\n",
    "        ret = (time[:open] - time[:close]) / time[:open]\n",
    "        expected_return += ret\n",
    "    end\n",
    "    expected_return /= 100\n",
    "\n",
    "    if ismissing(expected_return) || isnan(expected_return) || isinf(expected_return)\n",
    "        println(\"stock dropped: $stock\")\n",
    "        continue\n",
    "    end\n",
    "\n",
    "    push!(mu, expected_return)\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b9035c",
   "metadata": {},
   "source": [
    "#### Optimization problem\n",
    "\n",
    "Optimizer: Quadratic, where the objective is $\\min(\\text{risk})$ while getting a certian expected return, this function can be represented as: $$\\text{Objective} = - \\mu^T x + \\lambda x^T \\Sigma x$$\n",
    "```julia\n",
    "lambda = 0.5\n",
    "model = Model(Ipopt.Optimizer)\n",
    "n_stocks = length(mu)\n",
    "\n",
    "@variable(model, x[1:n_stocks] >= 0)\n",
    "@constraint(model, sum(x) == 1)\n",
    "@objective(model, Min, -mu' * x + lambda * (x' * sigma * x))\n",
    "\n",
    "optimize!(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3283be54",
   "metadata": {},
   "source": [
    "#### Output to .txt/.csv\n",
    "This simply uses julias inbuilt function to output.\n",
    "```julia\n",
    "stocks_info = [(clean_stocks[i], value(x[i]), mu[i]) for i in 1:n_stocks]\n",
    "sorted_stocks = sort(stocks_info, by = x -> x[2], rev=true)\n",
    "\n",
    "# Write to CSV\n",
    "csv_filename = \"topstocks.csv\"\n",
    "CSV.write(csv_filename, DataFrame(Stock = [stock[1] for stock in sorted_stocks],\n",
    "                                 Allocation = [stock[2] for stock in sorted_stocks],\n",
    "                                 ExpectedReturn = [stock[3] for stock in sorted_stocks]))\n",
    "\n",
    "# Write to TXT\n",
    "txt_filename = \"topstocks.txt\"\n",
    "open(txt_filename, \"w\") do f\n",
    "    println(f, \"Top Stocks\")\n",
    "    for i in (length(sorted_stocks))\n",
    "        stock = sorted_stocks[i]\n",
    "        println(f, \"$i. $(stock[1]) - Estimated Return: $(stock[3])\")\n",
    "    end\n",
    "end\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
